\section{Active Learning}
\begin{framed}
    $\H{\rX \mid \rY} \defeq \E[\vy \sim p]{\H{\rX \mid \rY = \vy}} \\= \E[\vx, \vy \sim p]{- \log p(\vx \mid \vy)}$ \\
    $\H{\rX, \rY} \defeq \E[\vx, \vy \sim p]{- \log p(\vx, \vy)}$ \\
    $\H{\rX, \rY} = \H{\rY} + \H{\rX \mid \rY} = \H{\rX} + \H{\rY \mid \rX}$ \\
    $\H{\rX \mid \rY} = \H{\rY \mid \rX} + \H{\rX} - \H{\rY}$ (Bayes Rule) \\
    $\H{\rX \mid \rY} \leq \H{\rX}$ (Information never hurts)\\
    $\I{\rX}{\rY} \defeq \H{\rX} + \H{\rY} - \H{\rX, \rY}=\KL{p(x,y)}{p(x)p(y)}$\\
    $\I{\rX}{\rY}[\rZ] = \H{\rX \mid \rZ} - \H{\rX \mid \rY, \rZ}=\KL{p(x,y|z)}{p(x|z)p(y|z)}$. \\
    $\text{I}(\rX;\rY;\rZ) = \I{\rX}{\rZ} + \I{\rX}{\rY}[\rZ]$
\end{framed}
\textbf{MI of dependent Gaussians}: given $X\sim\N{\vmu}{\mSigma}$ and $Y=X+\varepsilon$ with $\varepsilon\sim\N{\vzero}{\sigma^2 \mI}$:
$\I{X}{Y}=\frac{1}{2}\log |\sigma^{-2}\Sigma+I|$
Given a (discrete) function $F : \pset{\spX} \to \R$:
\textbf{Marginal gain}: $\Delta_F(\vx \mid \sA) \defeq F(\sA \cup \{\vx\}) - F(\sA)$\\
\textbf{Submodular}: $\forall \vx \in \spX \forall \sA \subseteq \sB \subseteq \spX: \Delta_F(\vx \mid \sA) \geq \Delta_F(\vx \mid \sB)$.\\
\textbf{Monotone}: $\forall \sA \subseteq \sB: F(\sA) \leq F(\sB)$.\\
$\text{I}$ is \textbf{monotone submodular}.\\
\textbf{Uncertainty sampling}: 
$\vx_{t+1} \defeq \argmax_{\vx \in \spX} \Delta_I(\vx \mid \sS_t)$\\
$= \argmax_{\vx \in \spX} \Ism{f_\vx}{y_{\vx} \mid \vy_{\sS_t}}$\\
$=\arg\max_{\vx \in \spX}\log\left(1+\frac{\sigma_t^2(\vx)}{\sigman(\vx)^2}\right)$
\textbf{Greedy} maximization of $\text{I}$ is a $(1 - \nicefrac{1}{e})$-approximation of the optimum.
Does not work with heteroscedastic noise: fails to distinguish between sources of uncertainty.
\begin{framed}
    \textbf{Bayesian active learning by disagreement (BALD)}: $\vx_{t+1} \defeq \argmax_{\vx \in \spX} \I{\vtheta}{y_{\vx} \mid \vx_{1:t}, y_{1:t}} = \argmax_{\vx \in \spX} \H{y_{\vx} \mid \vx_{1:t}, y_{1:t}} - \E*[\vtheta \mid \vx_{1:t}, y_{1:t}]{\H{y_{\vx} \mid \vtheta}}$ 
\end{framed}
\textbf{Transductive learning}: $x_{t+1}=\arg\max_{x \in \spX} \I{f_x^\star}{y_x \mid \vx_{1:t}, y_{1:t}}$
