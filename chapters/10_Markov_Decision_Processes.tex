\section{Markov Decision Processes}
\begin{framed}
    A \textbf{(finite) Markov decision process} is specified by a (finite) set of \textbf{states} $\sX \defeq \{1, \dots, n\}$; a (finite) set of \textbf{actions} $\sA \defeq \{1, \dots, m\}$; \textbf{transition probabilities} ${p(x' \mid x, a) \defeq \Pr{X_{t+1} = x' \mid X_t = x, A_t = a}}$; a \textbf{reward function} $r : X \times A \to \R$ which maps the current state $x$ and an action $a$ to some \textbf{reward}.
\end{framed}
$r$ induces a sequence of rewards: $R_t \defeq r(X_t, A_t)$.
\begin{framed}
    A \textbf{policy} is a function that maps each state $x \in \sX$ to a probability distribution over the actions. That is, for any $t > 0$: $\pi(a \mid x) \defeq \Pr{A_t = a \mid X_t = x}$.
\end{framed}
A policy induces a MC $(X_t^\pi)_{t\in\Nat_0}$\\
\textbf{Discounted payoff}: $G_t \defeq \sum_{m=0}^\infty \gamma^m R_{t+m}$, $\gamma \in [0, 1)$ is the \textbf{discount factor}.\\
\begin{framed}
    \textbf{State value function}: $v^\pi_t(x) \defeq \E[\pi]{G_t \mid X_t = x}$\\
    \textbf{State-action value function (Q-function)}: $q^\pi_t(x, a) \defeq \E[\pi]{G_t \mid X_t = x, A_t = a}$\\
    \textbf{Bellman Expectation Equation}: \\$\v[\pi]{x} = r(x, \pi(x)) + \gamma \E[x' \mid x, \pi(x)]{\v[\pi]{x'}}$\\
    For stochastic policies: $\v[\pi]{x}=\E[a \sim \pi(x)]{\q[\pi]{x}{a}}$
\end{framed}

Can be used to find $\fnv[\pi]$ given policy $\pi$, by solving linear system of equations in $\BigO{n^3}$. \\
\textbf{Fixed point iteration}: $\mB^\pi \vv \defeq \vr^\pi + \gamma \mP^\pi \vv$.
$\mB^\pi$ is contraction with contraction factor $\gamma<1$ $\implies$ unique optimal value function $\fnv[\star]$.
\begin{framed}
    \textbf{Greedy policy}: $\pi(x) \defeq \argmax_{a \in \sA} q^\pi_t(x, a)$\\
    \textbf{Bellman's Theorem}: A policy $\pis$ is optimal iff it is greedy w.r.t. its own value function.
\end{framed}
\textbf{Bellman optimality equations}:\\
    $ \fnv[\star](x) = \max_{a \in \sA} \fnq[\star](x,a)$\\
    $ \fnq[\star](x, a) = r(x, a) + \gamma \E[x' \mid x, a]{\max_{a' \in \sA} \fnq[\star](x', a')}$

\includegraphics[width=\linewidth, trim={0 0 3cm 0}]{images/Policy_Iteration.png}
For finite MDPs, policy iteration converges to an optimal policy (monotonic improvement).
\includegraphics[width=\linewidth, trim={0 0 3cm 0}]{images/Value_Iteration.png}
Value iteration to an $\epsilon$-optimal policy in polynomial time, as $\fnv[\star]$ and $\fnq[\star]$ are a fixed-points of the Bellman update $\mBs$.\\
$\fnv_t$ corresponds the the optimal value function assuming only $t$ steps are ever taken.
\begin{framed}
    A \textbf{Partially observable Markov decision process (POMDP)} is a Markov process with \textbf{hidden states}, 
    a set of supplementary \textbf{observations} $\sY$, 
    and \textbf{observation probabilities} $o(y \mid x) \defeq \Pr{Y_t = y \mid X_t = x}$.
    Given a POMDP, the corresponding \textbf{Belief-state Markov decision process} is a Markov decision process specified by the 
    \textbf{belief space} $\spB \defeq \Delta^{\sX}$; 
    the set of \textbf{actions} $\sA$; \textbf{transition probabilities} $\tau(b' \mid b, a) \defeq \Pr{B_{t+1} = b' \mid B_t = b, A_t = a}$;
    and \textbf{rewards} $\rho(b, a) \defeq \E[x \sim b]{r(x, a)} = \sum_{x \in \sX} b(x) r(x, a)$.\\
    $b_{t+1}(x) = \Pr{X_{t+1} = x \mid y_{1:t+1}, a_{1:t}} $ is deterministic.\\
    $\Pr{y_{t+1} \mid b_t, a_t}$\\
    $ = \E[x \sim b_t]{\E[x' \mid x, a_t]{\Pr{y_{t+1} \mid X_{t+1} = x'}}} $\\
    $= \sum_{x \in \sX} b_t(x) \sum_{x' \in \sX} p(x' \mid x, a_t) \cdot o(y_{t+1} \mid x')$.
\end{framed}