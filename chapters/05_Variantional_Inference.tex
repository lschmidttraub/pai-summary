\section{Variational Inference}
Approximate $p(\vtheta \mid \vx_{1:n}, y_{1:n})$ with $q_\vlambda(\vtheta)\in\spQ$\\
\textbf{Laplace Approximation}:
$q(\vtheta) \defeq \N[\vtheta]{\vthetahat}{\inv{\mLambda}} \propto \exp(\hat{\psi}(\vtheta))$, with $\vthetahat$ the mode 
and $\mLambda \defeq - \hes_\psi(\vthetahat) = - \hes_\vtheta \log p(\vtheta \mid \mathcal D) \bigl |_{\vtheta = \vthetahat}$. \\
Inference: $p(\ys \mid \vxs,\mathcal D)  \approx \int p(\ys \mid \vxs, \vtheta) q_\vlambda(\vtheta) \,d\vtheta$.
\begin{framed}
    \textbf{Suprise}: $\S{u} \defeq - \log u$ (convex).\\
    \textbf{Entropy}:$\H{p} \defeq \E[x \sim p]{\S{p(x)}}$.\\
    \textbf{Cross-entropy}: $\crH{p}{q} \defeq \E[x \sim p]{\S{q(x)}}$.\\
    \textbf{KL divergence}: $\KL{p}{q} \defeq \crH{p}{q} - \H{p} = \E[x \sim p]{\log \frac{p(x)}{q(x)}}$.\\
\textbf{Gaussian}: $\H{\N{\vmu}{\mSigma}} = \frac{1}{2} \log \parentheses*{(2 \pi e)^d \det{\mSigma}}$.
\end{framed}
$\KL{p}{q} \geq 0$ (Gibbs); $\KL{p}{q} = 0$ iff $p = q $ almost everywhere.
$\N{\vmu}{\mSigma}$ has the \textbf{highest entropy} among all distributions mean $\vmu$ and variance $\mSigma$. \\
$\KL{\Bern{p}}{\Bern{q}} = p \log \frac{p}{q} + (1-p) \log \frac{(1-p)}{(1-q)}$  \\
\begin{framed}
    \textbf{Gaussian KL}: ${p \defeq \N{\vmu_p}{\mSigma_p}}$, ${q \defeq \N{\vmu_q}{\mSigma_q}}$: \\
    $\KL{p}{q} = \frac{1}{2} (\mathrm{tr}(\inv{\mSigma_q} \mSigma_p)$\\
    $+ \transpose{(\vmu_p - \vmu_q)} \inv{\mSigma_q} (\vmu_p - \vmu_q) \phantom{\frac12}  - d + \log \frac{\det{\mSigma_q}}{\det{\mSigma_p}}).$
\end{framed}
\textbf{Forward KL}: $\qs_1 \defeq \argmin_{q \in \spQ} \KL{p}{q}$;\\
minimize forward KL by \textbf{moment matching}
\textbf{Reverse KL}: $\qs_2 \defeq \argmin_{q \in \spQ} \KL{q}{p}$.\\
Reverse KL tends to greedily select the mode and underestimate the variance.
\begin{framed}
    \textbf{Evidence lower bound (ELBO)}\\
    $L(q,p;\mathcal D)=p(\mathcal D)-\text{KL}(q\|p(\cdot|\mathcal D))$\\
    $= \mathbb E_{\theta\sim q}[p(\mathcal D|\theta)]-\text{KL}(q\|p)$
\end{framed}
\textbf{Reparametrization trick}: For $\vepsilon \sim \phi$ independent of $\vlambda$ s.t. $\theta = \vg(\vepsilon; \vlambda)$, then: $\E[\vtheta \sim q_\vlambda]{\vf(\vtheta)} = \E[\vepsilon \sim \phi]{\vf(\vg(\vepsilon; \vlambda))}$.
For ELBO: $\grad_\vlambda \E[\vtheta \sim q_\vlambda]{\vf(\vtheta)} = \E[\vepsilon \sim \phi]{\grad_\vlambda \vf(\vg(\vepsilon; \vlambda))}$. \\
\textbf{Gaussian}: $q_\vlambda(\vtheta) \defeq \N[\vtheta]{\vmu}{\mSigma}$; ${\vepsilon \sim \SN}$, set: $\vtheta = \vg(\vepsilon; \vlambda) \defeq \msqrt{\mSigma} \vepsilon + \vmu$, then: $\phi(\vepsilon) = q_\vlambda(\vtheta) \cdot \abs{\det{\msqrt{\mSigma}}}$ and $\vepsilon = \inv{\vg}(\vtheta; \vlambda) = \mSigma^{-\nicefrac{1}{2}}(\vtheta - \vmu)$
