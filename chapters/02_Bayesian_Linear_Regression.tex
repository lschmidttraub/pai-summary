\section{Bayesian Linear Regression}
\textbf{Setting}: $\vy=\mX \vw + \epsilon$, $\epsilon \sim \N{\vzero}{\sigman^2 \mI}$\\
\textbf{Prior}: $\vw \sim \N{\vzero}{\sigmap^2 \mI}$\\
\textbf{Posterior}: $\vw \mid \mathcal D\sim \N{\vmu}{\mSigma}$, with $\mSigma \defeq \inv{\parentheses*{\sigman^{-2} \transpose{\mX} \mX + \sigmap^{-2} \mI}}$ and $\vmu \defeq \sigman^{-2} \mSigma \transpose{\mX} \vy$\\
MAP: $\vwhat_\MAP = \argmin_\vw \norm{\vy - \mX \vw}_2^2 + \frac{\sigman^2}{\sigmap^2} \norm{\vw}_2^2$, \textit{identical to ridge regression} with $\lambda \defeq \sigman^2 / \sigmap^2$. \\
A \textbf{Laplace prior} on the weights is equivalent to \textbf{lasso regression} with decay ${\lambda \defeq \sigman^2 / \ell}$. \\
\textbf{Inference}:$\ys \mid \vxs, \mathcal D\sim \N{\transpose{\vmu} \vxs}{\transpose{\vxs} \mSigma \vxs + \sigman^2}$. \\
$\Var{\ys \mid \vxs} = \underbrace{\E[\vtheta]{\Var[\ys]{\ys \mid \vxs, \vtheta}}}_{\text{aleatoric uncertainty}} + \underbrace{\Var[\vtheta]{\E[\ys]{\ys \mid \vxs, \vtheta}}}_{\text{epistemic uncertainty}}$.\\
$\vf \mid \mX \sim \N{\mPhi \E{\vw}}{\mPhi \Var{\vw} \transpose{\mPhi}} = \N{\vzero}{\mK}$, with $\mK = \sigmap^2 \mPhi \transpose{\mPhi}$\\
\textbf{Kernel-function}: $k(\vx, \vxp) \defeq \sigmap^2 \cdot \transpose{\vphi(\vx)} \vphi(\vxp) = \Cov{f(\vx), f(\vxp)}$.
\begin{framed}
    \textbf{Linear}: $k(\vx, \vxp) = l \transpose{\vx} \vxp$ \\
    \textbf{RBF/Gaussian}: $k(\vx, \vxp) = \exp{-\frac{(\vx - \vxp)^2}{2\sigma_p^2}}$ \\
    \textbf{Polynomial}: $k(\vx, \vxp) = (1 + \transpose{\vx} \vxp)^d$ \\
    \textbf{Laplacian}: $k(\vx, \vxp) = \exp{\left(-\alpha \norm{\vx - \vxp}\right)}$
\end{framed}
\begin{framed}
    \textbf{Properties of Kernels}:  \\
    $\mK_{\sA\sA}$ is \textbf{symmetric} and \textbf{p.s.d.} \\
    \textbf{Composition}: addition, multiplication, and composition with a function $f$ with positive coefficients in Taylor expansion.
    \textbf{Bochner's Theorem}: A continuous kernel on $\R^d$ is p.s.d iff its Fourier transform $p(\vomega)$ is non-negative.
\end{framed}
\textbf{Stationary}: $k(\vx, \vxp) = k(\vx-\vxp)$\\
\textbf{Isotropic}: $k(\vx, \vxp) = \tilde{k}(\norm{\vx-\vxp}_2)$.
\textbf{Cost}: $\BigO{d^3+nd^2}$, can be performed online with cost $\BigO{d^2}$ per iteration.