\section{Gaussian Processes}
\begin{framed}
    \textbf{Gaussian process} = infinite set of random variables s.t. any finite number of them are jointly Gaussian.\\
    $f\sim \GP{\mu}{k}$ with $\mu : \spX \to \R$, $k : \spX \times \spX \to \R$\\
    $\forall\sA \defeq \{\vx_1, \dots, \vx_m\} \subseteq \spX: \vf_\sA  \sim \N{\vmu_\sA}{\mK_{\sA\sA}}$ 
\end{framed}
\begin{framed}
    \textbf{Maximize Marginal Likelihood}: \\
    $ \vthetahat_\MLE \defeq \argmax_{\vtheta} p(y_{1:n} \mid \vx_{1:n}, \vtheta) \\= \argmax_{\vtheta} \int p(y_{1:n} \mid \vx_{1:n}, f, \vtheta) p(f \mid \vtheta) \,d f$.
\end{framed}
\textbf{Posterior}: 
$f \mid \mathcal D \sim \GP{\mu'}{k'}$, $
  \mu'(\vx) \defeq \mu(\vx) + \transpose{\vk_{\vx,\sA}} \inv{(\mK_{\sA\sA} + \sigman^2 \mI)} (\vy_\sA - \vmu_\sA)$,\\
  $k'(\vx, \vxp) \defeq k(\vx, \vxp) - \transpose{\vk_{\vx,\sA}} \inv{(\mK_{\sA\sA} + \sigman^2 \mI)} \vk_{\vxp,\sA}$
For GP-Regression ($y_{1:n} \mid \vx_{1:n}, \vtheta \sim \N{\vzero}{\mK_{f,\vtheta} + \sigman^2 \mI}$), write $\mK_{\vy,\vtheta} \defeq \mK_{f,\vtheta} + \sigman^2 \mI$, and obtain: $\vthetahat_\MLE = \argmin_{\vtheta} \frac{1}{2} \transpose{\vy} \inv{\mK_{\vy,\vtheta}} \vy + \frac{1}{2} \log \det{\mK_{\vy,\vtheta}}$. Also:
$\pdv{}{\theta_j} \log p(y_{1:n} \mid \vx_{1:n}, \vtheta) = \frac{1}{2} \tr{(\valpha \transpose{\valpha} - \inv{\mK_{\vy,\vtheta}}) \pdv{\mK_{\vy,\vtheta}}{\theta_j}}$.\\
\textbf{Cost}: $\BigO{n^3}$\\
\textbf{Local methods}: Only condition on $\vxp$ where $\abs{k(\vx, \vxp)} \geq \tau$.\\
\textbf{Kernel Approximation}: Construct a low dimensional feature map $\vphi : \R^d \to \R^m$ that approximates the kernel: $k(\vx, \vxp) \approx \transpose{\vphi(\vx)} \vphi(\vxp)$, then apply BLR\\
\textbf{Random Fourier features}: given $k$ stationary, \\
$p(\vomega)=\int_{\R^d} k(\vxi) e^{- i \transpose{\vxi} \vomega} \,d\vxi$\\ 
$k(\vx-\vxp) = \int_{\R^d} p(\vomega) e^{i \transpose{\vomega} (\vx-\vxp)} \,d\vomega=\mathbb E_{\omega\sim p,b\sim\mathcal U([0,2\pi])}[2\cos(\omega^Tx+b)\cos(\omega^Ty+b)]$.\\
$\phi(x)=\frac{1}{\sqrt{m}}[z_{\omega_1,b_1}(x),\ldots,z_{\omega_m,b_m}(x)]^T$ with $z_{\omega,b}(x)=\sqrt{2}\cos(\omega^Tx+b)$
\begin{framed}
    \textbf{Inducing points}: Use a subset $k$ training points as inducing points and approximate the kernel matrix with a low rank approximation.
    \textbf{SoR}: assume 0 covariance\\
    \textbf{FITC}: assume diagonal covariance\\ 
    \textbf{Runtime}: $\BigO{nk^2}$
\end{framed}
Choosing kernel parameters: maximize marginal likelihood
$\hat\theta=\arg\max_{\theta}\int p(y_{1:n}|f,x_{1:n},\theta)p(f|\theta)df$