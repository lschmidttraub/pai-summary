\section{Markov Chains}
\begin{framed}
    A \textbf{Markov Chain} over $\sS \defeq \{0, \dots, n-1\}$, is a sequence $(X_t)_{t \in \Nat_0} \in \sS$, such that the \textbf{Markov property}: $X_{t+1} \perp X_{0:t-1} \mid X_t$ is satisfied. 
\end{framed}
\textbf{Time-homogeneous} if there exists $p(x' \mid x) \defeq \Pr{X_{t+1} = x' \mid X_t = x}$, with transition matrix $P_{ij} = p(x_j \mid x_i)$. Each row sums up to 1. \\
The state of a MC at $t$ is a probability distribution $q_t$: $q_{t+1} = q_t \mP$.
\begin{framed}
    A distribution $\pi$ is \textbf{stationary} iff $\vpi = \vpi \mP$.
\end{framed}
\begin{framed}
    \textbf{Irreducible}: $\forall x, x' \in \sS\exists t \in \Nat: p^{(t)}(x' \mid x) > 0$.\\
    \textbf{Aperiodic}: $\forall x \in \sS\exists t_0 \in \Nat\forall t \geq t_0: p^{(t)}(x \mid x) > 0$.
    \textbf{Ergodic}: $\exists t \in \Nat: \forall x, x' \in \sS: p^{(t)}(x' \mid x) > 0$.    
\end{framed}
Irreducible MC $\rightarrow$ ergodic MC use: $\mP' = \frac{1}{2}\mP + \frac{1}{2}\mI$
\begin{framed}
    An ergodic MC has a unique stat. dist. $\pi$ (with full support) and $\lim_{t\to\infty} q_t = \pi$, independently of $q_0$.
\end{framed}
\begin{framed}
    \textbf{Detailed balance equation}: \\
    $\forall x, x' \in \sS: \pi(x) p(x' \mid x) = \pi(x') p(x \mid x')$
    $\implies$MC is \textbf{reversible} w.r.t. $\pi$ $\implies$ $\pi$ is \textbf{stationary}.
\end{framed}
\begin{framed}
    \textbf{Ergodic theorem} For an ergodic MC and a stat. dist. $\pi$ as well as $f : \sS \to \R$:
    $\frac{1}{n} \sum_{i=1}^n f(x_i) \almostsurely \sum_{x \in S} \pi(x) f(x) = \E[x \sim \pi]{f(x)}$, for $n\to\infty$ where $x_i \sim X_i \mid x_{i-1}$.
\end{framed}
\textbf{Acceptance distribution (Metropolis-Hastings)}: $\Bern{\alpha(\vxp \mid \vx)}$ where $\alpha(\vxp \mid \vx) \defeq \min \braces*{1, \frac{q(\vxp) r(\vx \mid \vxp)}{q(\vx) r(\vxp \mid \vx)}}$ to decide whether to follow the proposal yields a Markov chain with stationary distribution $p(\vx) = \frac{1}{Z} q(\vx)$.
The stationary distribution of the simulated Markov chain is $p(\vx)$.
\textbf{Gibbs distribution}: $p(\vx) = \frac{1}{Z} \exp(- f(\vx))$, $f$ is the \textbf{energy function}.\\
$f$ convex $\implies$ $p$ \textbf{log-concave}.\\
$\alpha(\vxp \mid \vx) = \min \braces*{1, \frac{r(\vx \mid \vxp)}{r(\vxp \mid \vx)} \exp(f(\vx) - f(\vxp))}$. \\
$\S{p(\vx)} = f(\vx) + \log Z$
\begin{framed}
    \textbf{MALA/LMC}: Shift the proposal distribution perpendicularly to the gradient of the energy function: $r(\vxp \mid \vx) = \N[\vxp]{\vx - \eta_t \grad f(\vx)}{2 \eta_t \mI}$.
    \textbf{ULA}: Unadjusted Langevin Algorithm (MALA with $\alpha(\vxp \mid \vx) = 1$).\\
    \textbf{SGLD}: approximate gradient of ULA with unbiased estimator\\
    \textbf{HMC}: lift samples up to a higher dimension and use Hamiltonian dynamics to sample from the target distribution.
\end{framed}
\begin{framed}
    \textbf{Diffusion}: Simulate Gaussian noising process as MC and learn backward process.
    $q(x_t \mid x_{t-1}) = \mathcal N(x_{t};\sqrt{ 1-\beta_{t} }x_{t-1},\beta_{t}I)$\\
    $q(x_t\mid x_0)=\mathcal N(x_{t};\sqrt{ \bar\alpha_{t} }x_{0}, 1-\bar\alpha_{t} ), \bar{\alpha}_{t}=\prod_{j=1}^t (1-\beta_{j})$\\
    $q(x_{t-1}\mid x_t, x_0)=\mathcal N(x_{t-1};\mu'_{t}(x_{t},x_{0}),\beta_{t}'I)$\\
    $\mu'_{t}(x_t,x_0)=\frac{(1-\bar{\alpha}_{t-1})\sqrt{ \alpha_{t} } }{1-\bar{\alpha}_{t} }x_{t}+ \frac{\beta_{t}\sqrt{ \bar{\alpha}_{t-1} } }{1-\bar\alpha_{t} }x_{0}$\\
    $\beta_t'=\sigma_t^2$\\
    $L_1=\log p_{\lambda}(x_{0}|x_{1})$\\
    $L_t=\text{KL}(q(\cdot|x_{t},x_{0})\|p_{\lambda}(\cdot|x_{t}))$
\end{framed}